{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLHytevMLAa6"
      },
      "outputs": [],
      "source": [
        "import mne\n",
        "\n",
        "file_path = '/project/joelvoss/RAM/FR2/' #file template\n",
        "\n",
        "#all the data subjects:\n",
        "list_of_all_subjects = ['R1001P','R1016M','R1023J','R1028M','R1035M','R1050M','R1060M','R1077T','R1112M','R1177M',\n",
        "                        'R1002P','R1018P','R1024E','R1030J','R1036M','R1052E','R1069M','R1085C','R1115T','R1184M',\n",
        "                        'R1003P','R1020J','R1026D','R1031M','R1042M','R1053M','R1070T','R1101T','R1150J','R1006P',\n",
        "                        'R1022J','R1027J','R1033D','R1048E','R1056M','R1074M','R1111M','R1176M']\n",
        "\n",
        "important = ['events_s0.json','rec_word_eeg_s0-epo.fif','word_eeg_s0-epo.fif',\n",
        "             'word_eeg_s1-epo.fif','events_s1.json','rec_word_eeg_s1-epo.fif','pairs.h5']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I first performed the permutation cluster test to understand if there is actually statistical differences in neural data between different memory task success!\n",
        "Here are the resources I used to help me:\n",
        "https://mne.tools/stable/generated/mne.stats.permutation_cluster_test.html\n",
        "https://mne.tools/stable/generated/mne.time_frequency.tfr_morlet.html"
      ],
      "metadata": {
        "id": "0u_apN2RM7b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Permutation Cluster Test on RECALLED vs NOT RECALLED on non-stimulation data\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from mne.stats import permutation_cluster_test\n",
        "\n",
        "def power(epochs,average=False):\n",
        "    frequencies = np.arange(1.5, 250, 3)\n",
        "    tfr_epochs_1 = mne.time_frequency.tfr_morlet(\n",
        "        epochs, n_cycles=2, return_itc=False, freqs=frequencies, decim=3, average=average\n",
        "    )\n",
        "\n",
        "    return tfr_epochs_1\n",
        "\n",
        "def permutations(number):\n",
        "    vals = np.fromiter(data_test5['is_stim'].values(), dtype=float)\n",
        "\n",
        "    isnt_stim = vals == 0\n",
        "\n",
        "    is_recalled = np.fromiter(data_test5['recalled'].values(), dtype=float) == 1\n",
        "\n",
        "    is_recalled = is_recalled[:number]\n",
        "\n",
        "    all_power = power(test_5_part1_epochs)\n",
        "\n",
        "    for ch in np.arange(all_power.data.shape[1]): #check channels on this dimension\n",
        "        F_obs, clusters, cluster_p_values, H0 = permutation_cluster_test(\n",
        "            [all_power.data[(is_recalled==0), ch, :, :], all_power.data[(is_recalled==1), ch, :, :]],\n",
        "            out_type=\"mask\",\n",
        "            n_permutations=1000,\n",
        "            threshold=2,\n",
        "            tail=0,\n",
        "            seed=np.random.default_rng(seed=8675309))\n",
        "\n",
        "\n",
        "for subject in list_of_all_subjects:\n",
        "    no_recall = []\n",
        "    recall = []\n",
        "\n",
        "    if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/events_s0.json'):\n",
        "        if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s0-epo.fif'):\n",
        "            events_file_path = '/project/joelvoss/RAM/FR2/' + subject + '/events_s0.json'\n",
        "            test_5_part1_epochs = mne.read_epochs('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s0-epo.fif')\n",
        "            number = len(test_5_part1_epochs.get_data())\n",
        "            with open(events_file_path, 'r') as file:\n",
        "                data_test5 = json.load(file)\n",
        "            print('----------------------------------------------')\n",
        "            print(subject)\n",
        "            permutations(number)"
      ],
      "metadata": {
        "id": "f6padZFbLEUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I test the hypothesis of how all individuals have the same memory networks (which they don't as seen from the AUC score of 0.51)."
      ],
      "metadata": {
        "id": "Kgae-UEWNY5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train a multivariate-classifier on not stimulated data for when something is recalled or not\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from mne.stats import permutation_cluster_test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "import numpy as np\n",
        "from scipy.signal import welch\n",
        "from scipy.integrate import simps\n",
        "\n",
        "def bandpower(data, band, sf=500, relative=False):\n",
        "    # Use Welch's method for PSD estimation\n",
        "    f, welch_psd = welch(data, sf, nperseg=len(data))  # You may adjust nperseg as needed\n",
        "\n",
        "    # Extract the frequency range of interest\n",
        "    low, high = band\n",
        "    idx_band = np.logical_and(f >= low, f <= high)\n",
        "\n",
        "    # Integral approximation of the spectrum using parabola (Simpson's rule)\n",
        "    bp = simps(welch_psd[idx_band], dx=f[1] - f[0])\n",
        "\n",
        "    if relative:\n",
        "        bp /= simps(welch_psd, dx=f[1] - f[0])\n",
        "\n",
        "    return bp\n",
        "\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "y = []\n",
        "X = []\n",
        "subject_channels = []\n",
        "\n",
        "for subject in list_of_all_subjects:\n",
        "    if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/events_s0.json'):\n",
        "        print('------------------------------------------------------------------')\n",
        "        print('------------------------------------------------------------------')\n",
        "        print(subject)\n",
        "        if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s0-epo.fif'):\n",
        "            events_file_path = '/project/joelvoss/RAM/FR2/' + subject + '/events_s0.json'\n",
        "\n",
        "            with open(events_file_path, 'r') as file:\n",
        "                data_test5 = json.load(file)\n",
        "\n",
        "            test_5_dict = {'type': {str(k): v for k, v in data_test5['type'].items() if (v == 'WORD')}} #want type==WORD and is_stim==0\n",
        "            test_5_directory = list(test_5_dict['type'].keys())\n",
        "\n",
        "            test_5_part1_epochs = mne.read_epochs('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s0-epo.fif')\n",
        "\n",
        "            list_of_channels = list(test_5_part1_epochs.ch_names)\n",
        "            for i in range(len(test_5_directory)):\n",
        "                if data_test5['is_stim'][test_5_directory[i]] == 0:\n",
        "                    if data_test5['recalled'][test_5_directory[i]] == 0:\n",
        "                        channel_data = []\n",
        "                        delta = []\n",
        "                        theta = []\n",
        "                        alpha = []\n",
        "                        beta = []\n",
        "                        gamma = []\n",
        "                        high_freq = []\n",
        "                        for channel_number in range(len(list_of_channels)):\n",
        "                            delta.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[0.5,4]))\n",
        "                            theta.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[4,8]))\n",
        "                            alpha.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[8,13]))\n",
        "                            beta.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[13,35]))\n",
        "                            gamma.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[35,100]))\n",
        "                            high_freq.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[70,200]))\n",
        "                        #FFT\n",
        "                        #delta 0.5-4, theta 4-8, alpha 8-13, beta 13-35, gamma 35-100, high freq 70-200\n",
        "                        channel_data.append(Average(delta))\n",
        "                        channel_data.append(Average(theta))\n",
        "                        channel_data.append(Average(alpha))\n",
        "                        channel_data.append(Average(beta))\n",
        "                        channel_data.append(Average(gamma))\n",
        "                        channel_data.append(Average(high_freq))\n",
        "                        X.append(channel_data)\n",
        "                        y.append(0)\n",
        "                    else:\n",
        "                        channel_data = []\n",
        "\n",
        "                        delta = []\n",
        "                        theta = []\n",
        "                        alpha = []\n",
        "                        beta = []\n",
        "                        gamma = []\n",
        "                        high_freq = []\n",
        "                        for channel_number in range(len(list_of_channels)):\n",
        "                            delta.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[0.5,4]))\n",
        "                            theta.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[4,8]))\n",
        "                            alpha.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[8,13]))\n",
        "                            beta.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[13,35]))\n",
        "                            gamma.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[35,100]))\n",
        "                            high_freq.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[70,200]))\n",
        "\n",
        "                        channel_data.append(Average(delta))\n",
        "                        channel_data.append(Average(theta))\n",
        "                        channel_data.append(Average(alpha))\n",
        "                        channel_data.append(Average(beta))\n",
        "                        channel_data.append(Average(gamma))\n",
        "                        channel_data.append(Average(high_freq))\n",
        "                        #FFT\n",
        "                        #delta 0.5-4, theta 4-8, alpha 8-13, beta 13-35, gamma 35-100, high freq 70-200\n",
        "                        X.append(channel_data)\n",
        "                        y.append(1)\n",
        "\n",
        "    if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/events_s1.json'):\n",
        "        if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s1-epo.fif'):\n",
        "            events_file_path = '/project/joelvoss/RAM/FR2/' + subject + '/events_s1.json'\n",
        "\n",
        "            with open(events_file_path, 'r') as file:\n",
        "                data_test5 = json.load(file)\n",
        "\n",
        "            test_5_dict = {'type': {str(k): v for k, v in data_test5['type'].items() if (v == 'WORD')}} #want type==WORD and is_stim==0\n",
        "            test_5_directory = list(test_5_dict['type'].keys())\n",
        "\n",
        "            test_5_part1_epochs = mne.read_epochs('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s1-epo.fif')\n",
        "\n",
        "            list_of_channels = list(test_5_part1_epochs.ch_names)\n",
        "            for i in range(len(test_5_directory)):\n",
        "                if data_test5['is_stim'][test_5_directory[i]] == 0:\n",
        "                    if data_test5['recalled'][test_5_directory[i]] == 0:\n",
        "                        channel_data = []\n",
        "                        delta = []\n",
        "                        theta = []\n",
        "                        alpha = []\n",
        "                        beta = []\n",
        "                        gamma = []\n",
        "                        high_freq = []\n",
        "                        for channel_number in range(len(list_of_channels)):\n",
        "                            delta.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[0.5,4]))\n",
        "                            theta.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[4,8]))\n",
        "                            alpha.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[8,13]))\n",
        "                            beta.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[13,35]))\n",
        "                            gamma.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[35,100]))\n",
        "                            high_freq.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[70,200]))\n",
        "\n",
        "                        channel_data.append(Average(delta))\n",
        "                        channel_data.append(Average(theta))\n",
        "                        channel_data.append(Average(alpha))\n",
        "                        channel_data.append(Average(beta))\n",
        "                        channel_data.append(Average(gamma))\n",
        "                        channel_data.append(Average(high_freq))\n",
        "                        #FFT\n",
        "                        #delta 0.5-4, theta 4-8, alpha 8-13, beta 13-35, gamma 35-100, high freq 70-200\n",
        "                        X.append(channel_data)\n",
        "                        y.append(0)\n",
        "                    else:\n",
        "                        channel_data = []\n",
        "                        delta = []\n",
        "                        theta = []\n",
        "                        alpha = []\n",
        "                        beta = []\n",
        "                        gamma = []\n",
        "                        high_freq = []\n",
        "                        for channel_number in range(len(list_of_channels)):\n",
        "                            delta.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[0.5,4]))\n",
        "                            theta.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[4,8]))\n",
        "                            alpha.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[8,13]))\n",
        "                            beta.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[13,35]))\n",
        "                            gamma.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[35,100]))\n",
        "                            high_freq.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[50:-50],[70,200]))\n",
        "\n",
        "                        channel_data.append(Average(delta))\n",
        "                        channel_data.append(Average(theta))\n",
        "                        channel_data.append(Average(alpha))\n",
        "                        channel_data.append(Average(beta))\n",
        "                        channel_data.append(Average(gamma))\n",
        "                        channel_data.append(Average(high_freq))\n",
        "                        #FFT\n",
        "                        #delta 0.5-4, theta 4-8, alpha 8-13, beta 13-35, gamma 35-100, high freq 70-200\n",
        "                        X.append(channel_data)\n",
        "                        y.append(1)\n",
        "    subject_channels.append(list_of_channels)\n",
        "\n",
        "data = pd.DataFrame(data=np.c_[X, y], columns=['delta','theta','alpha','beta','gamma','high freq','target'])\n",
        "\n",
        "# Assuming 'target' is your target variable column\n",
        "class_counts = data['target'].value_counts()\n",
        "\n",
        "# Find the class with the maximum count\n",
        "max_class_count = class_counts.max()\n",
        "\n",
        "# Resample each class to have the same count as the maximum class count\n",
        "resampled_data = pd.DataFrame()\n",
        "\n",
        "for class_label, count in class_counts.items():\n",
        "    class_data = data[data['target'] == class_label]\n",
        "    resampled_class_data = resample(class_data, replace=True, n_samples=max_class_count, random_state=42)\n",
        "    resampled_data = pd.concat([resampled_data, resampled_class_data])\n",
        "\n",
        "# Shuffle the resampled data\n",
        "data = resampled_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "#print(data['target'])\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :-1], data.iloc[:, -1], test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating a logistic regression model with L2 penalization\n",
        "logreg = LogisticRegression(penalty='l2', random_state=42)\n",
        "\n",
        "# Training the model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "y_prob = logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "coefficients = logreg.coef_[0]\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Creating a DataFrame to display feature importance\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "feature_importance = feature_importance.sort_values(by='Coefficient', ascending=True)\n",
        "\n",
        "# Displaying feature importance\n",
        "print(\"Feature Importance:\")\n",
        "print(feature_importance)\n",
        "\n",
        "print('------------------------')\n",
        "print('When under the assumption that everyone has similar brain processes during memory modulation')\n",
        "universal_model = logreg"
      ],
      "metadata": {
        "id": "lKwSoAVKLIUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I now adapt to each individual's unique brain pathways during memory. I get very good results!\n",
        "\n",
        "Here is a resource I used to help me:\n",
        "https://raphaelvallat.com/bandpower.html\n",
        "- I also emailed with Dr. Vallat (owner of the website above) with additional questions about periodogram analysis"
      ],
      "metadata": {
        "id": "OeBL0kHmNtcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train a multivariate-classifier on not stimulated data for when something is recalled or not\n",
        "\n",
        "def pad_or_truncate(seq, max_len):\n",
        "    return seq[:max_len] + [0] * (max_len - len(seq))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def modulation_algorithm(X, y, channels_info, return_model=False):\n",
        "    final_list = []\n",
        "    brain_waves = ['delta', 'theta', 'alpha', 'beta', 'gamma', 'high freq']\n",
        "\n",
        "    for a in channels_info:\n",
        "        for b in brain_waves:\n",
        "            final_list.append(b + ' ' + a)\n",
        "    final_list.append('target')\n",
        "\n",
        "    # Find the maximum length of inner lists in X\n",
        "    max_len = max(len(inner_list) for inner_list in X)\n",
        "\n",
        "    # Pad or truncate inner lists to make them consistent\n",
        "    X_padded = [pad_or_truncate(inner_list, max_len) for inner_list in X]\n",
        "\n",
        "    # Convert X (list of lists) to a 2D array\n",
        "    X_array = np.array(X_padded)\n",
        "\n",
        "    data = pd.DataFrame(data=np.c_[X_array, y], columns=final_list)\n",
        "\n",
        "    # Assuming 'target' is your target variable column\n",
        "    class_counts = data['target'].value_counts()\n",
        "\n",
        "    # Find the class with the maximum count\n",
        "    max_class_count = class_counts.max()\n",
        "\n",
        "    # Resample each class to have the same count as the maximum class count\n",
        "    resampled_data = pd.DataFrame()\n",
        "\n",
        "    for class_label, count in class_counts.items():\n",
        "        class_data = data[data['target'] == class_label]\n",
        "        resampled_class_data = resample(class_data, replace=True, n_samples=max_class_count, random_state=42)\n",
        "        resampled_data = pd.concat([resampled_data, resampled_class_data])\n",
        "\n",
        "    # Shuffle the resampled data\n",
        "    data = resampled_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Splitting the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :-1], data.iloc[:, -1], test_size=0.2, random_state=42)\n",
        "\n",
        "    # Creating a Random Forest Regressor model\n",
        "    rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "    # Training the model\n",
        "    rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Making predictions on the test set\n",
        "    y_pred = rf_regressor.predict(X_test)\n",
        "\n",
        "    # Evaluating the model\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "    print(f\"R^2 Score: {r2:.2f}\")\n",
        "\n",
        "    # Plotting predictions vs actual values\n",
        "    plt.scatter(y_test, y_pred)\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title('Actual vs Predicted Values')\n",
        "    plt.show()\n",
        "\n",
        "    # Feature importances from Random Forest Regressor\n",
        "    feature_importances = rf_regressor.feature_importances_\n",
        "\n",
        "    # Creating a DataFrame to display feature importance\n",
        "    feature_importance = pd.DataFrame({'Feature': data.columns[:-1], 'Importance': feature_importances})\n",
        "    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Displaying feature importance\n",
        "    print(\"Feature Importance:\")\n",
        "    print(feature_importance)\n",
        "\n",
        "    if return_model:\n",
        "        return rf_regressor\n",
        "\n",
        "algorithms_per_user = []\n",
        "user_directory_for_algorithms = []\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "print('------------------------')\n",
        "print('user-specific memory modulation prediction algorithms')\n",
        "\n",
        "for subject in list_of_all_subjects:\n",
        "    if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/events_s0.json'):\n",
        "        print('------------------------------------------------------------------')\n",
        "        print('------------------------------------------------------------------')\n",
        "        print(subject)\n",
        "        if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s0-epo.fif'):\n",
        "            events_file_path = '/project/joelvoss/RAM/FR2/' + subject + '/events_s0.json'\n",
        "\n",
        "            with open(events_file_path, 'r') as file:\n",
        "                data_test5 = json.load(file)\n",
        "\n",
        "            test_5_dict = {'type': {str(k): v for k, v in data_test5['type'].items() if (v == 'WORD')}} #want type==WORD and is_stim==0\n",
        "            test_5_directory = list(test_5_dict['type'].keys())\n",
        "\n",
        "            #print(test_5_directory)\n",
        "            test_5_part1_epochs = mne.read_epochs('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s0-epo.fif')\n",
        "\n",
        "            list_of_channels = list(test_5_part1_epochs.ch_names)\n",
        "            for i in range(len(test_5_directory)):\n",
        "                if data_test5['is_stim'][test_5_directory[i]] == 0:\n",
        "                    if data_test5['recalled'][test_5_directory[i]] == 0:\n",
        "                        channel_data = []\n",
        "                        for channel_number in range(len(list_of_channels)):\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[0.5,4]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[4,8]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[8,13]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[13,35]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[35,100]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[70,200]))\n",
        "                        #FFT\n",
        "                        #delta 0.5-4, theta 4-8, alpha 8-13, beta 13-35, gamma 35-100, high freq 70-200\n",
        "                        X.append(channel_data)\n",
        "                        y.append(0)\n",
        "                    else:\n",
        "                        channel_data = []\n",
        "                        for channel_number in range(len(list_of_channels)):\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[0.5,4]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[4,8]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[8,13]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[13,35]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[35,100]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[70,200]))\n",
        "                        #FFT\n",
        "                        #delta 0.5-4, theta 4-8, alpha 8-13, beta 13-35, gamma 35-100, high freq 70-200\n",
        "                        X.append(channel_data)\n",
        "                        y.append(1)\n",
        "\n",
        "    if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/events_s1.json'):\n",
        "        if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s1-epo.fif'):\n",
        "            events_file_path = '/project/joelvoss/RAM/FR2/' + subject + '/events_s1.json'\n",
        "\n",
        "            with open(events_file_path, 'r') as file:\n",
        "                data_test5 = json.load(file)\n",
        "\n",
        "            test_5_dict = {'type': {str(k): v for k, v in data_test5['type'].items() if (v == 'WORD')}} #want type==WORD and is_stim==0\n",
        "            test_5_directory = list(test_5_dict['type'].keys())\n",
        "\n",
        "            test_5_part1_epochs = mne.read_epochs('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s1-epo.fif')\n",
        "\n",
        "            list_of_channels = list(test_5_part1_epochs.ch_names)\n",
        "            for i in range(len(test_5_directory)):\n",
        "                if data_test5['is_stim'][test_5_directory[i]] == 0:\n",
        "                    if data_test5['recalled'][test_5_directory[i]] == 0:\n",
        "                        channel_data = []\n",
        "                        for channel_number in range(len(list_of_channels)):\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[0.5,4]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[4,8]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[8,13]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[13,35]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[35,100]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[70,200]))\n",
        "                        #FFT\n",
        "                        #delta 0.5-4, theta 4-8, alpha 8-13, beta 13-35, gamma 35-100, high freq 70-200\n",
        "                        X.append(channel_data)\n",
        "                        y.append(0)\n",
        "                    else:\n",
        "                        channel_data = []\n",
        "                        for channel_number in range(len(list_of_channels)):\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[0.5,4]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[4,8]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[8,13]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[13,35]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[35,100]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[70,200]))\n",
        "                        #FFT\n",
        "                        #delta 0.5-4, theta 4-8, alpha 8-13, beta 13-35, gamma 35-100, high freq 70-200\n",
        "                        X.append(channel_data)\n",
        "                        y.append(1)\n",
        "    print('HHHIIIII')\n",
        "    if len(X) != 0:\n",
        "        print('---------')\n",
        "        print(subject)\n",
        "        user_directory_for_algorithms.append(subject)\n",
        "        print('---------')\n",
        "        algorithms_per_user.append(modulation_algorithm(X, y, list_of_channels, return_model=True))\n",
        "    y = []\n",
        "    X = []"
      ],
      "metadata": {
        "id": "7OpoDPBILbiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the user-specific regression algorithms are good, I connected it towards the DBS success algorithm! Here, I determine if DBS can increase the memory performance based on the initial encoding."
      ],
      "metadata": {
        "id": "6kiQTx2ZOLO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "#Use the multivariate-classifier trained earlier to predict in the first 0.5 seconds of stimulation data if the\n",
        "#encoding state is good or bad, and then correlate these results with which brain region was stimulated and if\n",
        "#that memory task was properly encoded or not\n",
        "\n",
        "#algorithms_per_user = []\n",
        "#user_directory_for_algorithms = []\n",
        "\n",
        "def find_model(data_subject):\n",
        "    for i in range(len(algorithms_per_user)):\n",
        "        if user_directory_for_algorithms[i] == data_subject:\n",
        "            return algorithms_per_user[i]\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "round_X = []\n",
        "round_y = []\n",
        "\n",
        "for user in user_directory_for_algorithms:\n",
        "    #before the brain is stimulated, there is a 0.5 second duration break\n",
        "    #HOWEVER 2 consecutive words always get stimulated\n",
        "    count = 0\n",
        "    #This means that I should just take the first 0.5 seconds of the very first word to get iEEG data before stim\n",
        "    #Then I should take the last 0.5 seconds of iEEG from the brain data of the next epoch to see the effects\n",
        "    #Append regressor modulation prediction on pre-stim data and post-stim data\n",
        "    #Also append final result on if it was actually predicted or not\n",
        "\n",
        "    if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/events_s0.json'):\n",
        "        if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s0-epo.fif'):\n",
        "            events_file_path = '/project/joelvoss/RAM/FR2/' + subject + '/events_s0.json'\n",
        "\n",
        "            with open(events_file_path, 'r') as file:\n",
        "                data_test5 = json.load(file)\n",
        "\n",
        "            test_5_dict = {'type': {str(k): v for k, v in data_test5['type'].items() if (v == 'WORD')}} #want type==WORD and is_stim==0\n",
        "            test_5_directory = list(test_5_dict['type'].keys())\n",
        "\n",
        "            test_5_part1_epochs = mne.read_epochs('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s0-epo.fif')\n",
        "\n",
        "            list_of_channels = list(test_5_part1_epochs.ch_names)\n",
        "            for i in range(len(test_5_directory)):\n",
        "                if data_test5['is_stim'][test_5_directory[i]] == 1:\n",
        "                    if count%2 == 0:\n",
        "                        print('we')\n",
        "                        channel_data = []\n",
        "                        for channel_number in range(len(list_of_channels)):\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[0.5,4]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[4,8]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[8,13]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[13,35]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[35,100]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[70,200]))\n",
        "\n",
        "                        new_data = pd.DataFrame(channel_data).T\n",
        "\n",
        "                        logreg = find_model(subject)\n",
        "                        # Use the trained model to make predictions on the new data\n",
        "                        predictions = logreg.predict(new_data)\n",
        "\n",
        "                        round_X.append(predictions)\n",
        "                        #round_X.append(universal_model.predict(new_data))\n",
        "\n",
        "                        #FFT\n",
        "                        #delta 0.5-4, theta 4-8, alpha 8-13, beta 13-35, gamma 35-100, high freq 70-200\n",
        "                        #round_X.append(channel_data)\n",
        "                    else:\n",
        "                        print('are')\n",
        "                        channel_data = []\n",
        "                        for channel_number in range(len(list_of_channels)):\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[-250:],[0.5,4]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[-250:],[4,8]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[-250:],[8,13]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[-250:],[13,35]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[-250:],[35,100]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[-250:],[70,200]))\n",
        "\n",
        "                        new_data = pd.DataFrame(channel_data).T\n",
        "\n",
        "                        logreg = find_model(subject)\n",
        "                        # Use the trained model to make predictions on the new data\n",
        "                        predictions = logreg.predict(new_data)\n",
        "\n",
        "                        round_X.append(predictions)\n",
        "                        #round_X.append(universal_model.predict(new_data))\n",
        "\n",
        "                        #FFT\n",
        "                        #delta 0.5-4, theta 4-8, alpha 8-13, beta 13-35, gamma 35-100, high freq 70-200\n",
        "                        #round_X.append(channel_data)\n",
        "                        round_y.append(data_test5['recalled'][test_5_directory[i]])\n",
        "                        round_y.append(subject)\n",
        "\n",
        "                        X.append(round_X)\n",
        "                        y.append(round_y)\n",
        "\n",
        "                        round_X = []\n",
        "                        round_y = []\n",
        "\n",
        "                    count += 1\n",
        "\n",
        "    if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/events_s1.json'):\n",
        "        if os.path.isfile('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s1-epo.fif'):\n",
        "            events_file_path = '/project/joelvoss/RAM/FR2/' + subject + '/events_s1.json'\n",
        "\n",
        "            with open(events_file_path, 'r') as file:\n",
        "                data_test5 = json.load(file)\n",
        "\n",
        "            test_5_dict = {'type': {str(k): v for k, v in data_test5['type'].items() if (v == 'WORD')}} #want type==WORD and is_stim==0\n",
        "            test_5_directory = list(test_5_dict['type'].keys())\n",
        "\n",
        "            test_5_part1_epochs = mne.read_epochs('/project/joelvoss/RAM/FR2/' + subject + '/word_eeg_s1-epo.fif')\n",
        "\n",
        "            list_of_channels = list(test_5_part1_epochs.ch_names)\n",
        "            for i in range(len(test_5_directory)):\n",
        "                if data_test5['is_stim'][test_5_directory[i]] == 1:\n",
        "                    if count%2 == 0:\n",
        "                        print('we')\n",
        "                        channel_data = []\n",
        "                        for channel_number in range(len(list_of_channels)):\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[0.5,4]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[4,8]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[8,13]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[13,35]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[35,100]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[0:250],[70,200]))\n",
        "\n",
        "                        new_data = pd.DataFrame(channel_data).T\n",
        "\n",
        "                        logreg = find_model(subject)\n",
        "                        # Use the trained model to make predictions on the new data\n",
        "                        predictions = logreg.predict(new_data)\n",
        "\n",
        "                        round_X.append(predictions)\n",
        "                        #round_X.append(universal_model.predict(new_data))\n",
        "\n",
        "                        #FFT\n",
        "                        #delta 0.5-4, theta 4-8, alpha 8-13, beta 13-35, gamma 35-100, high freq 70-200\n",
        "                        #round_X.append(channel_data)\n",
        "                    else:\n",
        "                        print('are')\n",
        "                        channel_data = []\n",
        "                        for channel_number in range(len(list_of_channels)):\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[-250:],[0.5,4]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[-250:],[4,8]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[-250:],[8,13]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[-250:],[13,35]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[-250:],[35,100]))\n",
        "                            channel_data.append(bandpower(test_5_part1_epochs[i].get_data()[0][channel_number].flatten()[-250:],[70,200]))\n",
        "\n",
        "                        new_data = pd.DataFrame(channel_data).T\n",
        "\n",
        "                        logreg = find_model(subject)\n",
        "                        # Use the trained model to make predictions on the new data\n",
        "                        predictions = logreg.predict(new_data)\n",
        "\n",
        "                        round_X.append(predictions)\n",
        "                        #round_X.append(universal_model.predict(new_data))\n",
        "\n",
        "                        #FFT\n",
        "                        #delta 0.5-4, theta 4-8, alpha 8-13, beta 13-35, gamma 35-100, high freq 70-200\n",
        "                        #round_X.append(channel_data)\n",
        "                        round_y.append(data_test5['recalled'][test_5_directory[i]])\n",
        "                        round_y.append(subject)\n",
        "\n",
        "                        X.append(round_X)\n",
        "                        y.append(round_y)\n",
        "\n",
        "                        round_X = []\n",
        "                        round_y = []\n",
        "\n",
        "                    count += 1\n",
        "\n",
        "saved_X = X\n",
        "saved_y = y\n",
        "\n",
        "for a in range(len(saved_X)):\n",
        "    saved_X[a] = [saved_X[a][0][0],saved_X[a][1][0]]\n",
        "\n",
        "for a in range(len(saved_y)):\n",
        "    saved_y[a] = saved_y[a][0]\n",
        "\n",
        "data = pd.DataFrame(data=np.c_[saved_X, saved_y], columns=['user specific prediction prestimulus','user specific prediction poststimulus','target'])\n"
      ],
      "metadata": {
        "id": "5ll5ASvGLiQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displayed AUC and Confusion Matrix:"
      ],
      "metadata": {
        "id": "-NBqK_zjPydL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve, confusion_matrix\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Assuming 'target' is your target variable column\n",
        "class_counts = data['target'].value_counts()\n",
        "\n",
        "# Find the class with the maximum count\n",
        "max_class_count = class_counts.max()\n",
        "\n",
        "# Resample each class to have the same count as the maximum class count\n",
        "resampled_data = pd.DataFrame()\n",
        "\n",
        "for class_label, count in class_counts.items():\n",
        "    class_data = data[data['target'] == class_label]\n",
        "    resampled_class_data = resample(class_data, replace=True, n_samples=max_class_count, random_state=42)\n",
        "    resampled_data = pd.concat([resampled_data, resampled_class_data])\n",
        "\n",
        "# Shuffle the resampled data\n",
        "data = resampled_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Recalculate X using only the 1st feature of data\n",
        "X = data.iloc[:, 0].values.reshape(-1, 1)\n",
        "y = data['target']\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Training the model\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "\n",
        "# Computing ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Plotting the ROC curve with shading\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.fill_between(fpr, 0, tpr, color='blue', alpha=0.3)\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Save the ROC curve plot\n",
        "plt.savefig('roc_curve.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Making binary predictions based on a threshold (you can choose a threshold based on your preference)\n",
        "threshold = 0.5\n",
        "y_pred_binary = (y_pred_proba > threshold).astype(int)\n",
        "\n",
        "# Plotting the confusion matrix with actual numbers\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', cbar=False,\n",
        "            xticklabels=['Predicted No Recall', 'Predicted Recall'], yticklabels=['Actual No Recall', 'Actual Recall'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZqXvY0OxPxUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also graphed the effects of DBS before and after in different memory encoding states\n"
      ],
      "metadata": {
        "id": "18mfzFmIPnbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Assuming 'target' is your target variable column\n",
        "class_counts = data['target'].value_counts()\n",
        "\n",
        "# Find the class with the maximum count\n",
        "max_class_count = class_counts.max()\n",
        "\n",
        "# Resample each class to have the same count as the maximum class count\n",
        "resampled_data = pd.DataFrame()\n",
        "\n",
        "for class_label, count in class_counts.items():\n",
        "    class_data = data[data['target'] == class_label]\n",
        "    resampled_class_data = resample(class_data, replace=True, n_samples=max_class_count, random_state=42)\n",
        "    resampled_data = pd.concat([resampled_data, resampled_class_data])\n",
        "\n",
        "# Shuffle the resampled data\n",
        "data = resampled_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Recalculate X using the 1st and 2nd features of data\n",
        "X = data.iloc[:, :2]\n",
        "\n",
        "# Plotting the points with grid lines and y=x line\n",
        "plt.figure(figsize=(12, 8))  # Adjust the figsize to your preference\n",
        "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], label='Data Points', alpha=0.7)\n",
        "\n",
        "# Adding grid lines\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Adding y=x line\n",
        "plt.plot([X.iloc[:, 0].min(), X.iloc[:, 1].max()], [X.iloc[:, 0].min(), X.iloc[:, 1].max()], color='black', linestyle='--', label='y=x')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('pre-stimulus memory modulation prediction')\n",
        "plt.ylabel('post-stimulus memory modulation prediction')\n",
        "plt.title('Effects of Brain Stimulation on Memory Modulation State')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_2kvLAIrPl0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to do line of best fit between pre-stimulus memory encoding state and recall success"
      ],
      "metadata": {
        "id": "wdF0ecbTP2aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "# Assuming 'target' is your target variable column\n",
        "class_counts = data['target'].value_counts()\n",
        "\n",
        "# Find the class with the maximum count\n",
        "max_class_count = class_counts.max()\n",
        "\n",
        "# Resample each class to have the same count as the maximum class count\n",
        "resampled_data = pd.DataFrame()\n",
        "\n",
        "for class_label, count in class_counts.items():\n",
        "    class_data = data[data['target'] == class_label]\n",
        "    resampled_class_data = resample(class_data, replace=True, n_samples=max_class_count, random_state=42)\n",
        "    resampled_data = pd.concat([resampled_data, resampled_class_data])\n",
        "\n",
        "# Shuffle the resampled data\n",
        "data = resampled_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Plotting the points\n",
        "plt.figure(figsize=(12, 8))  # Adjust the figsize to your preference\n",
        "plt.scatter(data.iloc[:, 0], data['target'], label='Data Points', alpha=0.7)\n",
        "\n",
        "# Fitting a polynomial curve to the data\n",
        "def func(x, a, b, c):\n",
        "    return a * x**2 + b * x + c\n",
        "\n",
        "params, _ = curve_fit(func, data.iloc[:, 0], data['target'])\n",
        "x_fit = np.linspace(data.iloc[:, 0].min(), data.iloc[:, 0].max(), 100)\n",
        "y_fit = func(x_fit, *params)\n",
        "\n",
        "# Plotting the best fit curve\n",
        "plt.plot(x_fit, y_fit, color='red', label=f'Best Fit Curve: {params[0]:.2f}x^2 + {params[1]:.2f}x + {params[2]:.2f}')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('prestimulus memory modulation prediction')\n",
        "plt.xlim(0,1)\n",
        "plt.ylabel('recall success')\n",
        "plt.ylim(0,1)\n",
        "plt.title('Scatter Plot with Best Fit Curve')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1SldURxYL2_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}